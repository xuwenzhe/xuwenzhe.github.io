<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Wenzhe&#39;s Blog</title>
        <link>https://xuwenzhe.github.io/</link>
        <description>About LoveIt Theme</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Thu, 03 Sep 2020 01:08:40 -0700</lastBuildDate>
            <atom:link href="https://xuwenzhe.github.io/index.xml" rel="self" type="application/rss+xml" />
        <item>
    <title>Spark 101: DataFrame基本使用</title>
    <link>https://xuwenzhe.github.io/spark-practice-101/</link>
    <pubDate>Thu, 03 Sep 2020 01:08:40 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-practice-101/</guid>
    <description><![CDATA[Example 1. DataFrame的5种基本操作 1.筛选行，2.选取列，3.增加行列，4.分组总结，5.排序
一个简单的工资数据集：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  df = spark.createDataFrame([ (1, &#39;sales&#39;, 4200), (2, &#39;admin&#39;, 3100), (3, &#39;sales&#39;, 4000), (4, &#39;sales&#39;, 4000), (5, &#39;admin&#39;, 2700), (6, &#39;dev&#39;, 3400), (7, &#39;dev&#39;, 5200), (8, &#39;dev&#39;, 3700), (9, &#39;dev&#39;, 4400), (10, &#39;dev&#39;, 4400) ], schema=[&#39;id&#39;, &#39;dept&#39;, &#39;salary&#39;]) df.]]></description>
</item><item>
    <title>Spark: 解决数据倾斜的N种姿势</title>
    <link>https://xuwenzhe.github.io/spark-data-skew/</link>
    <pubDate>Sun, 30 Aug 2020 02:45:00 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-data-skew/</guid>
    <description><![CDATA[什么是数据倾斜？ 对于Spark/Hadoop等分布式系统，机器间工作量的不均匀比单纯的工作量大更难处理，因为单纯的工作量大基本通过scale-out就可以解决。假设有100个任务并行处理，若任务间工作量相当，20台机器要比10台机器耗时减半（假设机器性能相当）。然而当任务间工作量相差巨大时（例如，任务中10个大任务，90个小任务，工作量差别1M倍），很难通过scale-out的方式充分利用并行优势，此时，完成总耗时取决于最慢的大任务何时完成（木桶原理）。
对于Spark这种计算引擎，工作量主要在于所需处理的数据量。因此当任务划分时，数据分布不均匀，即发生了数据倾斜。
数据是如何倾斜的？ 当Spark产生shuffle操作时，相同key的数据会被放在相同的partition。因此当某一个key的数据量过大（即popular key），会造成partition间工作量巨大差别，造成数据倾斜。这样不仅影响了并行效率，更加容易造成内存不足，产生OOM（Out Of Memory）报错使整个应用崩溃。
常见会触发shuffle的算子：distinct, groupByKey, reduceByKey, aggregateByKey, join, cogroup, repartition。
如何识别数据倾斜？   观察stage进度条，发现大多数task很快执行完，但是剩余task等待时间巨长甚至出现OOM报错。（如上图） 查看Spark Web UI里的Shuffle Read Size/Records，查看任务间数据量分布是否均匀。 计算key的分布。（如下）  1 2 3 4  df.select(&#34;key&#34;).sample(false, 0.1) // 数据采样  .(k =&gt; (k, 1)).reduceBykey(_ + _) // 统计 key 出现的次数  .map(k =&gt; (k._2, k._1)).sortByKey(false) // 根据 key 出现次数进行排序  .take(10) // 取前 10 个。   发生了数据倾斜怎么办？ 思路1 - 过滤异常数据 如果数据包含大量异常key，过滤掉就好了。
异常key：
 空值 Null 无效数据，大量重复的测试数据或是对结果影响不大的有效数据 有效数据，业务导致的正常数据分布  对于情况1&amp;2，直接过滤即可。对于3，直接过滤不适用，尝试以下其他方法。]]></description>
</item><item>
    <title>Spark FAQ</title>
    <link>https://xuwenzhe.github.io/spark-fundamentals/</link>
    <pubDate>Sat, 29 Aug 2020 23:46:21 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-fundamentals/</guid>
    <description><![CDATA[什么是Job, Stage, Task? Spark数据处理是像流水线一样，通过一系列操作（算子）完成对RDD的处理, 即RDD1 -&gt; RDD2 -&gt; ...。操作分为两类：Transformation和Action。Spark使用lazy evaluation模式，即Transformation操作下达后，实际并不立刻运行，只有当遇到Action时才把之前所有的Transformation和当前Action运行完毕。每当一个Action触发，就会生成一个Job。因此Job以Action划分。Job之间是串行的，只有当前job结束，才会启动下一个job。
 一个Job会被划分为一个或多个stage。在一个stage中，任务是可以并行计算的。stage是按照ShuffleDependency来进行划分的。两种依赖方式为窄依赖（narrow dependency）与宽依赖（wide dependency）。区分两种依赖的方法是看：父RDD的partition是否被多个（&gt;1）子RDD的partition使用，若未被多个子RDD的partition使用，则为窄依赖，不需shuffle，不划分stage； 否则为宽依赖，划分stage。上图对比了两种依赖方式的不同。具体划分stage的算法是: 从最后一个RDD开始，从后往前推，找该RDD和父RDD之间的依赖关系，如果是窄依赖，会继续找父RDD的父RDD，如果是宽依赖，就会从该RDD开始到前面所有的RDD划分为一个stage，递归的出口是直到找不到父RDD，最后把所有的RDD划分为一个stage。
一个例子：
 一个stage并行的任务称为task，对应一个partition的处理，即task总数为stage的partition总数。
参考 Wide vs Narrow Dependencies github blog
stackoverflow, Does stages in an application run parallel in spark
Queirozf Apache Spark Architecture Overview: Jobs, Stages, Tasks, etc
cnblogs, qingyunzong
csdn, Z_Data
Spark by Examples, Spark Repartition vs Coalesce
Arganzheng&rsquo;s Blog, Spark数据倾斜及其解决方案]]></description>
</item><item>
    <title>Linux入门</title>
    <link>https://xuwenzhe.github.io/linux-intro/</link>
    <pubDate>Wed, 19 Aug 2020 17:17:14 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/linux-intro/</guid>
    <description><![CDATA[目录结构  /bin (/usr/bin, /usr/local/bin): binary,存放最经常使用的命令 /sbin (/usr/sbin, /usr/local/sbin): s: super user, 存放系统管理员使用的系统管理程序 /home: 存放普通用户的主目录，每个用户都有一个自己的目录 /root: 超级权限者的用户主目录(This is not the root (/) filesystem. It is the home directory for the root user.) /lib: 系统开机所需要最基本的动态连接共享库，类似Windows的DLL文件。几乎所有应用程序都需要用到这些共享库 /etc: 系统管理所需要的配置文件和子目录 /usr: 用户的很多应用程序和文件都放在这个目录下，类似Windows的program files目录 /proc,/srv,/sys: 内核相关目录（不要轻易修改） /dev: 硬件以文件的形式存放在该处 /media: U盘，光驱，等等 /mnt: 用户临时挂载的文件系统 /opt: 存放安装软件 /usr/local: 存放安装软件所安装的目录，一般是通过编译源码的方式安装的程序 /var: 存放经常被修改的东西，比如日志文件   Vi/Vim快捷键  拷贝当前行yy, 拷贝当前行以下的5行5yy,粘贴p 删除当前行dd, 删除当前行以下的5行5dd 查找关键词/keyword,下一个出现n 显示行号:set nu,隐藏行号:set nonu 阅览最首行gg最末行G 撤销输入动作u 移动光标至第20行20+shift g  开关重启命令 1 2 3 4 5 6  sync # 把内存数据同步到磁盘，以下操作前先执行该命令 shutdown -h now # 立即关机 shutdown -h 1 # 1min 关机 shutdown -r now # 立即重启 halt # 等同于关机 reboot # 立即重启   用户管理 1 2 3 4 5 6 7 8 9 10  useradd xiaoming # 自动创建home下同名目录 passwd xiaoming # 指定密码 userdel xiaoming # 删除用户xiaoming，home下其目录会保留 userdel -r xiaoming # 删除用户xiaoming，home下其目录不会保留 su - xiaoming # 切换用户登陆， 权限不足会给提示，exit返回到原先用户 groupadd zuming # 创建组 groupdel zuming # 删除组 useradd -g zuming xiaoming # 创建用户时，指定组 id xiaoming # 查询用户信息 usermod -g zuming2 xiaoming # 修改用户所在组    用户配置文件在： /etc/passwd （用户id，组id，家目录，shell） 组配置文件在：/etc/group 口令配置文件（密码，登录信息）：/etc/shadow(加密的)  实用命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  man ls # 显示ls用法 help cd # 显示cd用法 touch filename # 创建空文件 cat -n filename ｜ more # 打开文件内容（只读方式）, -n 显示行号，more分页显示 more filename # 按页显示文件，空格翻页，Enter下一行，ctrl+f/b less filename # 分页查看，lazy加载，利于显示大文件 &#34;less is more&#34; ls -l &gt; filename # 重定向（覆盖） ls -l &gt;&gt; filename # 追加（append到尾部） head -n 5 filename # 只显示文件前5行（默认10行） tail -n 5 filename # 只显示文件后5行（默认10行） -f追踪文件更新，日志监控经常用 date &#34;+%Y-%m-%d %H:%M:%S&#34; # 按格式显示当前年月日时分秒，“+”必要， -s设定 cal # 显示当前日历 find [scope] [-name 文件名, -user 用户, -size 文件大小] grep [-n 显示匹配行行号， -i 忽略大小写] 查找内容 源文件 cat hello.]]></description>
</item><item>
    <title>Precision和Recall</title>
    <link>https://xuwenzhe.github.io/precision-recall/</link>
    <pubDate>Wed, 12 Aug 2020 22:00:55 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/precision-recall/</guid>
    <description><![CDATA[什么是精确率（precision）与召回率（recall）？ Precision和Recall是衡量模型预测能力的指标。在简单的分类问题（种类数=2）中，根据实际答案与模型预测，可分为如下4种情况：
 True Positive (TP) - 正确预测了阳类 True Negative (TN) - 正确预测了阴类 False Positive (FP) - 错误预测了阳类（实际为阴类） False Negative (FN) - 错误预测了阴类（实际为阳类）  注：True和False表示预测是否正确，Positive和Negative表示预测结果
精准率和召回率定义：
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
他们区别在分母不同，所取值范围均为0～1
如何理解Precision和Recall？ 信息检索中，Precision又叫查准率（找得对），Recall又叫查全率（找得全）。
举个例子，池塘有大龙虾30只，小龙虾70只。我们发明了一只捕大龙虾神器专捕大龙虾。神器一网下去，捕到大龙虾20只，小龙虾5只。那么：
精准率 = 20 /（20 + 5） = 0.80
召回率 = 20 / 30 = 0.67
我们来看3个极端神器的Precision和Recall。
 神器谨慎版：只捕了一只，且为大龙虾。Precision = 1/1 = 1.0, Recall = 1/30 = 0.]]></description>
</item><item>
    <title>网络广告是如何用cookie来追踪用户的？</title>
    <link>https://xuwenzhe.github.io/cookie-track-user/</link>
    <pubDate>Sun, 09 Aug 2020 23:51:42 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/cookie-track-user/</guid>
    <description><![CDATA[什么是Cookie 参考 Web客户端追踪(上)-Cookie追踪
网络广告代理商是如何通过 cookie 收集用户信息的？ - LO的回答 - 知乎
RTB广告竞价系统的算法介绍
你是如何被广告跟踪的？ - 巴伐利亚啤酒馆的文章 - 知乎
第一方Cookie和第三方Cookie
第一方Cookie与第三方Cookie的区别
第一方和第三方cookie是什么？
ETP,ITP,NO-TP,是时候把第三方Cookie讲清楚了]]></description>
</item><item>
    <title>从Pandas到PySpark</title>
    <link>https://xuwenzhe.github.io/from-pandas-to-pyspark/</link>
    <pubDate>Sun, 09 Aug 2020 22:25:25 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/from-pandas-to-pyspark/</guid>
    <description><![CDATA[PySpark的特性 Immutable
 Changes create new object references Old versions are unchanged  Lazy
 Compute does not happen until output is requested  Pandas VS PySpark Load CSV 1 2 3 4 5  # Pandas df = pd.read_csv(&#34;datafile.csv&#34;) # PySpark df = spark.read.options(header=True, inferSchema=True).csv(&#34;datafile.csv&#34;)   View Dataframe 1 2 3 4 5  # Pandas df # PySpark df.show()   Columns &amp; Data Types 1 2 3 4 5 6 7  # Pandas df.]]></description>
</item><item>
    <title>Git与Github使用</title>
    <link>https://xuwenzhe.github.io/git-github/</link>
    <pubDate>Tue, 28 Jul 2020 12:31:43 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/git-github/</guid>
    <description><![CDATA[The git directory acts as a database for all the changes tracked in Git and the working tree acts as a sandbox where we can edit the current versions of the files.
1. 基本操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  git init # 创建仓库 # 登陆当前操作系统的用户范围 (否则为项目级别) git config --global user.name &#34;My Name&#34; git config --global user.]]></description>
</item><item>
    <title>Leetcode：位运算</title>
    <link>https://xuwenzhe.github.io/leetcode-bit-manipulation/</link>
    <pubDate>Sun, 26 Jul 2020 23:50:06 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/leetcode-bit-manipulation/</guid>
    <description><![CDATA[while位 342. Power of Four (easy) 简述：判断是否为4的幂
思路：mask挪位置
联系：出自693
1 2 3 4 5 6 7 8 9 10 11 12 13  # class Solution(object): # def isPowerOfFour(self, num): # &#34;&#34;&#34; # :type num: int # :rtype: bool # &#34;&#34;&#34; # base = 1 while base &lt;= num: # if base == num: # return True # 要点1-1: 偶数位为1 base &lt;&lt;= 2 # return False   476. Number Complement (easy) 简述：将所给数二进制取反输出对应十进制]]></description>
</item><item>
    <title>Leetcode：树的Traversal</title>
    <link>https://xuwenzhe.github.io/leetcode-tree-traversal/</link>
    <pubDate>Thu, 16 Jul 2020 12:46:49 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/leetcode-tree-traversal/</guid>
    <description><![CDATA[In-Order 230. Kth Smallest Element in a BST (medium) 简述：在BST中，求第k小的值
思路：inorder，记录遍历idx
联系：模板题
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  # class Solution(object): # def kthSmallest(self, root, k): # &#34;&#34;&#34; # :type root: TreeNode # :type k: int # :rtype: int # &#34;&#34;&#34; # self.idx, self.res = 0, None # self.k = k # self.]]></description>
</item></channel>
</rss>
