<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>spark - Tag - Wenzhe&#39;s Blog</title>
        <link>https://xuwenzhe.github.io/tags/spark/</link>
        <description>spark - Tag - Wenzhe&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 30 Aug 2020 02:45:00 -0700</lastBuildDate><atom:link href="https://xuwenzhe.github.io/tags/spark/" rel="self" type="application/rss+xml" /><item>
    <title>Spark: N Ways to Optimize Data Skew</title>
    <link>https://xuwenzhe.github.io/spark-data-skew/</link>
    <pubDate>Sun, 30 Aug 2020 02:45:00 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-data-skew/</guid>
    <description><![CDATA[什么是数据倾斜？ 对于Spark/Hadoop等分布式系统，机器间工作量的不均匀比单纯的工作量大更难处理，因为单纯的工作量大基本通过scale-out就可以解决。假设有100个任务并行处理，若任务间工作量相当，20台机器要比10台机器耗时减半（假设机器性能相当）。然而当任务间工作量相差巨大时（例如，任务中10个大任务，90个小任务，工作量差别1M倍），很难通过scale-out的方式充分利用并行优势，此时，完成总耗时取决于最慢的大任务何时完成（木桶原理）。
对于Spark这种计算引擎，工作量主要在于所需处理的数据量。因此当任务划分时，数据分布不均匀，即发生了数据倾斜。
数据是如何倾斜的？ 当Spark产生shuffle操作时，相同key的数据会被放在相同的partition。因此当某一个key的数据量过大（即popular key），会造成partition间工作量巨大差别，造成数据倾斜。这样不仅影响了并行效率，更加容易造成内存不足，产生OOM（Out Of Memory）报错使整个应用崩溃。
常见会触发shuffle的算子：distinct, groupByKey, reduceByKey, aggregateByKey, join, cogroup, repartition。
如何识别数据倾斜？   观察stage进度条，发现大多数task很快执行完，但是剩余task等待时间巨长甚至出现OOM报错。（如上图） 查看Spark Web UI里的Shuffle Read Size/Records，查看任务间数据量分布是否均匀。 计算key的分布。（如下）  1 2 3 4  df.select(&#34;key&#34;).sample(false, 0.1) // 数据采样  .(k =&gt; (k, 1)).reduceBykey(_ + _) // 统计 key 出现的次数  .map(k =&gt; (k._2, k._1)).sortByKey(false) // 根据 key 出现次数进行排序  .take(10) // 取前 10 个。   发生了数据倾斜怎么办？ 思路1 - 过滤异常数据 如果数据包含大量异常key，过滤掉就好了。
异常key：
 空值 Null 无效数据，大量重复的测试数据或是对结果影响不大的有效数据 有效数据，业务导致的正常数据分布  对于情况1&amp;2，直接过滤即可。对于3，直接过滤不适用，尝试以下其他方法。]]></description>
</item><item>
    <title>Spark FAQ</title>
    <link>https://xuwenzhe.github.io/spark-fundamentals/</link>
    <pubDate>Sat, 29 Aug 2020 23:46:21 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-fundamentals/</guid>
    <description><![CDATA[什么是Job, Stage, Task? Spark数据处理是像流水线一样，通过一系列操作（算子）完成对RDD的处理, 即RDD1 -&gt; RDD2 -&gt; ...。操作分为两类：Transformation和Action。Spark使用lazy evaluation模式，即Transformation操作下达后，实际并不立刻运行，只有当遇到Action时才把之前所有的Transformation和当前Action运行完毕。每当一个Action触发，就会生成一个Job。因此Job以Action划分。Job之间是串行的，只有当前job结束，才会启动下一个job。
 一个Job会被划分为一个或多个stage。在一个stage中，任务是可以并行计算的。stage是按照ShuffleDependency来进行划分的。两种依赖方式为窄依赖（narrow dependency）与宽依赖（wide dependency）。区分两种依赖的方法是看：父RDD的partition是否被多个（&gt;1）子RDD的partition使用，若未被多个子RDD的partition使用，则为窄依赖，不需shuffle，不划分stage； 否则为宽依赖，划分stage。上图对比了两种依赖方式的不同。具体划分stage的算法是: 从最后一个RDD开始，从后往前推，找该RDD和父RDD之间的依赖关系，如果是窄依赖，会继续找父RDD的父RDD，如果是宽依赖，就会从该RDD开始到前面所有的RDD划分为一个stage，递归的出口是直到找不到父RDD，最后把所有的RDD划分为一个stage。
一个例子：
 一个stage并行的任务称为task，对应一个partition的处理，即task总数为stage的partition总数。
参考 Wide vs Narrow Dependencies github blog
stackoverflow, Does stages in an application run parallel in spark
Queirozf Apache Spark Architecture Overview: Jobs, Stages, Tasks, etc
cnblogs, qingyunzong
csdn, Z_Data
Spark by Examples, Spark Repartition vs Coalesce
Arganzheng&rsquo;s Blog, Spark数据倾斜及其解决方案]]></description>
</item><item>
    <title>从Pandas到PySpark</title>
    <link>https://xuwenzhe.github.io/from-pandas-to-pyspark/</link>
    <pubDate>Sun, 09 Aug 2020 22:25:25 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/from-pandas-to-pyspark/</guid>
    <description><![CDATA[PySpark的特性 Immutable
 Changes create new object references Old versions are unchanged  Lazy
 Compute does not happen until output is requested  Pandas VS PySpark Load CSV 1 2 3 4 5  # Pandas df = pd.read_csv(&#34;datafile.csv&#34;) # PySpark df = spark.read.options(header=True, inferSchema=True).csv(&#34;datafile.csv&#34;)   View Dataframe 1 2 3 4 5  # Pandas df # PySpark df.show()   Columns &amp; Data Types 1 2 3 4 5 6 7  # Pandas df.]]></description>
</item></channel>
</rss>
