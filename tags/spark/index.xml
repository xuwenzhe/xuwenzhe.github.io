<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>spark - Tag - Wenzhe&#39;s Blog</title>
        <link>https://xuwenzhe.github.io/tags/spark/</link>
        <description>spark - Tag - Wenzhe&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 30 Aug 2020 02:45:00 -0700</lastBuildDate><atom:link href="https://xuwenzhe.github.io/tags/spark/" rel="self" type="application/rss+xml" /><item>
    <title>Spark: Optimize Data Skew in N Ways</title>
    <link>https://xuwenzhe.github.io/spark-data-skew/</link>
    <pubDate>Sun, 30 Aug 2020 02:45:00 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-data-skew/</guid>
    <description><![CDATA[What is Data Skew? For distributed data computing engines like Spark/Hadoop, skewed data brings more pain than the big data. Ideally, for a cluster consisting of N machines, the wall time to finish an embarrassingly parallel job is reduced to 1/N. Unfortunately, perfect parallelism is not always achieved. In other words, the workload might not be evenly distributed across the cluster. For example, a machine might end up with computing 80% of the workload, while the other two machines are taking care of the remaining 20% of the workload (10% each).]]></description>
</item><item>
    <title>Spark FAQ</title>
    <link>https://xuwenzhe.github.io/spark-fundamentals/</link>
    <pubDate>Sat, 29 Aug 2020 23:46:21 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/spark-fundamentals/</guid>
    <description><![CDATA[什么是Job, Stage, Task? Spark数据处理是像流水线一样，通过一系列操作（算子）完成对RDD的处理, 即RDD1 -&gt; RDD2 -&gt; ...。操作分为两类：Transformation和Action。Spark使用lazy evaluation模式，即Transformation操作下达后，实际并不立刻运行，只有当遇到Action时才把之前所有的Transformation和当前Action运行完毕。每当一个Action触发，就会生成一个Job。因此Job以Action划分。Job之间是串行的，只有当前job结束，才会启动下一个job。
 一个Job会被划分为一个或多个stage。在一个stage中，任务是可以并行计算的。stage是按照ShuffleDependency来进行划分的。两种依赖方式为窄依赖（narrow dependency）与宽依赖（wide dependency）。区分两种依赖的方法是看：父RDD的partition是否被多个（&gt;1）子RDD的partition使用，若未被多个子RDD的partition使用，则为窄依赖，不需shuffle，不划分stage； 否则为宽依赖，划分stage。上图对比了两种依赖方式的不同。具体划分stage的算法是: 从最后一个RDD开始，从后往前推，找该RDD和父RDD之间的依赖关系，如果是窄依赖，会继续找父RDD的父RDD，如果是宽依赖，就会从该RDD开始到前面所有的RDD划分为一个stage，递归的出口是直到找不到父RDD，最后把所有的RDD划分为一个stage。
一个例子：
 一个stage并行的任务称为task，对应一个partition的处理，即task总数为stage的partition总数。
参考 Wide vs Narrow Dependencies github blog
stackoverflow, Does stages in an application run parallel in spark
Queirozf Apache Spark Architecture Overview: Jobs, Stages, Tasks, etc
cnblogs, qingyunzong
csdn, Z_Data
Spark by Examples, Spark Repartition vs Coalesce
Arganzheng&rsquo;s Blog, Spark数据倾斜及其解决方案]]></description>
</item><item>
    <title>从Pandas到PySpark</title>
    <link>https://xuwenzhe.github.io/from-pandas-to-pyspark/</link>
    <pubDate>Sun, 09 Aug 2020 22:25:25 -0700</pubDate>
    <author>Author</author>
    <guid>https://xuwenzhe.github.io/from-pandas-to-pyspark/</guid>
    <description><![CDATA[PySpark的特性 Immutable
 Changes create new object references Old versions are unchanged  Lazy
 Compute does not happen until output is requested  Pandas VS PySpark Load CSV 1 2 3 4 5  # Pandas df = pd.read_csv(&#34;datafile.csv&#34;) # PySpark df = spark.read.options(header=True, inferSchema=True).csv(&#34;datafile.csv&#34;)   View Dataframe 1 2 3 4 5  # Pandas df # PySpark df.show()   Columns &amp; Data Types 1 2 3 4 5 6 7  # Pandas df.]]></description>
</item></channel>
</rss>
